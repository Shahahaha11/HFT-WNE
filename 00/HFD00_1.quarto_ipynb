{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "author: \"Shah\"\n",
        "title: \"MS-GARCH : Three Regime Markov Switching GJR and FI GARCH\"\n",
        "date: 10/15/2025\n",
        "date-format: \"MMM D, YYYY\"\n",
        "format:\n",
        "#   docx: default\n",
        "  html: \n",
        "      code-fold: false\n",
        "jupyter: python3\n",
        "html-math-method: katex\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "Model : Two State MS GJR GARCH with Poisson Jumps\n",
        "First, to clarify what the possion jump does for this model. \n",
        "We add a jump term to the conditional variance before computing densities of the Markov switch. We are de-meaning the returns by the expected jump and adding jump variance on top of the diffusive variance. Our $h_t$ is the diffusion and not the entire variance itself.\n",
        "The model assumes mean :$u_t = r_t - \\lambda\\mu J$\n",
        "\n",
        "With this diffusion + jump set up. It seems like we add the jump term to the return (shock) part for likelihood but we just separate it to eat up the noise. We then model the rest of the series with the MS model.\n",
        "This gives the model a bigger variance apetite for higher volatility securities.\n",
        "\n",
        "## Data\n",
        "We use minutely data spanning 2025-09-03 to 2025-10-17 downloaded from yfinance."
      ],
      "id": "abcb141a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from arch import arch_model\n",
        "import pandas as pd\n",
        "import yfinance as yf \n",
        "from scipy.optimize import minimize\n",
        "import pandas as pd\n",
        "from math import sqrt, pi, exp\n",
        "\n",
        "# New ticker names for calling global variables. dash changed to underscore.\n",
        "tickers = [\"AAPL\",\"BP\",\"BRK_B\",\"BTC_USD\",\"CIFR\",\"CNTA\",\"COP\",\"CVX\", \"IREN\",\"JPM\",\"LUMN\",\"META\",\"MPC\",\"NVDA\",\"OXY\",\"PSX\",\"RIOT\", \"SHEL\",\"TSLA\",\"VLO\",\"XOM\"]\n",
        "\n",
        "# READING CONCATED DATA UNTIL G\n",
        "for t in tickers:\n",
        "    globals()[f\"d_{t}\"] = pd.read_pickle(f\"./G_concat_prices/G_{t}.pkl\")"
      ],
      "id": "8db71898",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for t in tickers:\n",
        "    data = globals()[f\"d_{t}\"]\n",
        "    split = int(len(data) * 80)\n",
        "    globals()[f\"train_{t}\"] = data[:split] \n",
        "    globals()[f\"test_{t}\"] = data[split:]"
      ],
      "id": "72ad9c6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for t in tickers:\n",
        "    globals()[f\"d_{t}\"]['Close'] = pd.to_numeric(globals()[f\"d_{t}\"]['Close'] , errors= \"coerce\")\n",
        "\n",
        "for t in tickers:\n",
        "    globals()[f\"y_{t}\"] = globals()[f\"d_{t}\"]['Close']\n",
        "\n",
        "for t in tickers:\n",
        "    arr = globals()[f\"y_{t}\"].to_numpy(dtype=float)   # convert to numeric array\n",
        "    globals()[f\"r_{t}\"] = pd.Series(np.log(arr)).diff().dropna() *100"
      ],
      "id": "bea9b529",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: false\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(pd.to_numeric(y_AAPL))\n",
        "plt.title(\"AAPL Close Price\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()\n"
      ],
      "id": "0ba02926",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: false\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(pd.to_numeric(r_AAPL))\n",
        "plt.title(\"AAPL Returns\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()"
      ],
      "id": "79750088",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MS GJR-GARCH with (Poisson Jumps) - Model\n",
        "\n",
        "### Regime equations (mdl03)\n",
        "$$\n",
        "h_{t+1}^{(0)} = \\omega_0 + (\\alpha_0 + \\gamma_0\\,\\mathbf{1}\\{u_t < 0\\})(u_t^2 + \\lambda\\sigma_J^2) + \\beta_0\\,h_t^{(0)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_{t+1}^{(1)} = \\omega_1 + (\\alpha_1 + \\gamma_1\\,\\mathbf{1}\\{u_t < 0\\})(u_t^2 + \\lambda\\sigma_J^2) + \\beta_1\\,h_t^{(1)}\n",
        "$$\n",
        "\n",
        "where $u_t = r_t - \\lambda\\mu_J$ represents the jump-adjusted innovation.\n",
        "\n",
        "Variance in likelihood is the sum of diffusive variance $h_t(j)$ plus jump variance.\n",
        "\n",
        "### Regime filter\n",
        "\n",
        "$$\n",
        "\\ell_j = f(r_t \\mid s_t=j)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\pi_{t|t}(j)=\\frac{\\pi_{t|t-1}(j)\\,\\ell_j}{\\sum_{k}\\pi_{t|t-1}(k)\\,\\ell_k}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\pi_{t+1|t}(j)=\\sum_i \\pi_{t|t}(i)P_{ij}\\quad\\text{(i.e., }\\pi_{t+1|t}=\\pi_{t|t}P\\text{)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\ell_j = f(r_t \\mid s_t=j)\n",
        "$$\n",
        "\n",
        "\n",
        "# Model Main\n",
        "\n",
        "## Setting up initial transition matrix and other important functions"
      ],
      "id": "ad7349d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "######### PLAIN MARKOV SWITCHING MODEL\n",
        "R = 2  # number of regimes\n",
        "P = np.array([[0.95, 0.05],   # transition matrix\n",
        "              [0.10, 0.90]])  # rows sum to 1\n",
        "\n",
        "pi_0 = np.full(R, 1.0/R)   # start equal across regimes\n",
        "\n",
        "# each regime j has [omega, alpha, beta]\n",
        "params = {\n",
        "    0: [0.01, 0.05, 0.90],\n",
        "    1: [0.02, 0.10, 0.80]\n",
        "}\n",
        "\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -500, 500)     # avoid overflow\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def row_softmax(M):\n",
        "    M = M - M.max(axis=1, keepdims=True)\n",
        "    E = np.exp(M)\n",
        "     # we substract the maximum in each value from all entries in that row \n",
        "     # so the largest becomes 0, then we exponentiate.\n",
        "     # that gives exp(0)=1 and others <1. to mimic softmax results but safely scaled.\n",
        "    return E / E.sum(axis=1, keepdims=True)\n",
        "\n",
        "def safe_sigmoid(x):\n",
        "    return np.exp(-np.logaddexp(0, -x)) \n",
        "\n",
        "def softplus(z):  # stable: log(1+e^z)\n",
        "    return np.log1p(np.exp(-np.abs(z))) + np.maximum(z, 0.0)"
      ],
      "id": "afb01d05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unpack Theta\n",
        "\n",
        "This functions defined all the initial variables and starting values. it takes a raw input of theta0 and transforms them to abide GARCH assumptions."
      ],
      "id": "95de3c39"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def unpack_theta(theta, R=2):\n",
        "    k = 4*R\n",
        "    w_raw, a_raw, b_raw, g_raw = theta[:R], theta[R:2*R], theta[2*R:3*R], theta[3*R:4*R]\n",
        "    P_logits = theta[k:k+R*R].reshape(R, R)\n",
        "    lam_raw, muJ, sJ_raw = theta[k+R*R:k+R*R+3]\n",
        "    # omega = np.exp(w_raw)\n",
        "    # omega = np.log1p(np.exp(w_raw))\n",
        "    omega = np.log1p(np.exp(-np.abs(w_raw))) + np.maximum(w_raw, 0)\n",
        "    # alpha = 1/(1+np.exp(-a_raw))\n",
        "    # alpha = 1 / (1 + np.exp(-0.5 * a_raw))\n",
        "    alpha = safe_sigmoid(0.5 * a_raw)\n",
        "    # beta  = sigmoid(b_raw)\n",
        "    # beta  = 1 / (1+ np.exp(-0.5* b_raw))\n",
        "    beta  = safe_sigmoid(0.5 * b_raw)\n",
        "    # gamma = sigmoid(g_raw) \n",
        "    gamma  = 1 / (1+ np.exp(-0.5* g_raw))\n",
        "    # gamma = np.exp(-np.log1p(np.exp(-g_raw)))\n",
        "    \n",
        "    # gamma[0] = 0 # This allows for the second regime to have no leverage effect\n",
        "    # single unified stationarity cap (incl. leverage)\n",
        "    s = alpha + beta + gamma\n",
        "    mask = s >= 0.999\n",
        "    if mask.any():\n",
        "        alpha[mask] *= 0.999 / s[mask]\n",
        "        beta[mask]  *= 0.999 / s[mask]\n",
        "        gamma[mask] *= 0.999 / s[mask]\n",
        "\n",
        "    # Markov transitions\n",
        "    P = np.exp(P_logits - P_logits.max(axis=1, keepdims=True))\n",
        "    P = P / P.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Jumps with stable positivity and caps\n",
        "    lam     = np.minimum(softplus(lam_raw), 5.0)          # lam ∈ (0,5]\n",
        "    sigmaJ  = np.minimum(softplus(sJ_raw),  1.0)          # sigma_J ∈ (0,1]\n",
        "    sigmaJ2 = sigmaJ*sigmaJ\n",
        "    return omega, alpha, beta, gamma, P, lam, muJ, sigmaJ2\n"
      ],
      "id": "92ee100a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _lgit(x): return np.log(x/(1-x))\n",
        "\n",
        "theta0 = np.r_[ np.log(0.01)*np.ones(R),\n",
        "                _lgit(0.05)*np.ones(R),\n",
        "                _lgit(0.90)*np.ones(R),\n",
        "                np.zeros(R),                  # γ\n",
        "                np.zeros(R*R),\n",
        "                np.log(0.1),   # lam_raw\n",
        "                0.0,           # muJ\n",
        "                np.log(0.02)   # sJ_raw\n",
        "              ]"
      ],
      "id": "35327ed9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NLL-MS-GARCH function\n",
        "This function specifies the two regimes and jump parameters to output a log likelihood of the parameters extracted from unpack_theta."
      ],
      "id": "1a253484"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def nll_ms_garch(theta, r, R=2, tol=1e-12):\n",
        "    r = np.asarray(r, float)\n",
        "    omega, alpha, beta, gamma, P, lam, muJ, sigmaJ2 = unpack_theta(theta, R)\n",
        "    pi = np.full(R, 1.0/R)\n",
        "    h  = (omega + alpha*lam*sigmaJ2) / (1 - alpha - beta + 1e-9)\n",
        "    h  = np.maximum(h, 1e-8)\n",
        "    ll = 0.0\n",
        "\n",
        "    for x in r:\n",
        "        u   = x - lam*muJ\n",
        "        var = np.maximum(h + lam*sigmaJ2, 1e-12)                 # include jump variance\n",
        "        dens = (1.0/np.sqrt(2*np.pi*var)) * np.exp(-0.5*(u*u)/var)\n",
        "        mix  = pi @ dens + tol\n",
        "        ll  += np.log(mix)\n",
        "        post = (pi * dens) / mix\n",
        "\n",
        "        neg = float(u < 0.0)\n",
        "        h0  = omega[0] + alpha[0]*(u*u + lam*sigmaJ2) + beta[0]*h[0]\n",
        "        h1  = omega[1] + (alpha[1] + gamma[1]*neg)*(u*u + lam*sigmaJ2) + beta[1]*h[1]\n",
        "        h   = np.array([h0, h1])\n",
        "\n",
        "        pi  = post @ P\n",
        "\n",
        "    return -ll"
      ],
      "id": "24b1610a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecast VAR Function\n",
        "This function unpacks the tuned set of parameters (pre-optimized) from the nll_ms_garch and outputs a k period ahead forecast."
      ],
      "id": "6662c6a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Poisson-jump recursive variance update (structural form)\n",
        "def forecast_var_k(r, theta, R=2, k=3):\n",
        "    omega, alpha, beta, gamma, P, lam, muJ, sigmaJ2 = unpack_theta(theta, R)\n",
        "    pi = np.full(R, 1.0/R)\n",
        "    h  = (omega + alpha*lam*sigmaJ2) / (1 - alpha - beta + 1e-9)\n",
        "    regime = []\n",
        "    for x in r:\n",
        "        Jt = lam * muJ\n",
        "        u  = x - Jt\n",
        "        neg = (u < 0).astype(float)\n",
        "        u2 = u*u\n",
        "        u2j= u2 + lam*sigmaJ2\n",
        "        h  = omega + alpha*u2 + gamma*u2j*neg + beta*h\n",
        "        pi = pi @ P # Expected h_t+1\n",
        "        regime.append(pi.copy())\n",
        "    out = []\n",
        "    for _ in range(k):\n",
        "        out.append(float(pi @ h))\n",
        "        h  = omega + alpha*out[-1] + beta*h\n",
        "        pi = pi @ P\n",
        "    \n",
        "    return np.array(out) , regime"
      ],
      "id": "39df0fb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rolling Forecast Function\n",
        "Rolling forecasts using k-7100 observations for convenience , inputting less data. \n",
        "We have rolling window of 7 observations ie. we observe past 7 minutes "
      ],
      "id": "1f7b2fc1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tickers = [\"AAPL\",\"BRK_B\",\"JPM\",\"LUMN\",\"MPC\",\"OXY\"]\n",
        "tickers = [\"AAPL\",\"BRK_B\"]\n",
        "yg_roll = []\n",
        "import matplotlib.pyplot as plt\n",
        "rolling_forecasts = {}\n",
        "rolling_residuals = {}\n",
        "for t in tickers:\n",
        "    r = np.asarray(globals()[f\"r_{t}\"], float)\n",
        "    f_roll = []\n",
        "    prev_theta = theta0.copy()\n",
        "    regime = []\n",
        "    for i in range(7100, len(r)):       \n",
        "        r_sub = r[i-50:i]   # forecast uses same r as in third last line r**2                \n",
        "        res = minimize(nll_ms_garch, prev_theta, args=(r_sub, 2), method='L-BFGS-B')\n",
        "        theta_i = prev_theta if (not res.success or not np.isfinite(res.fun)) else res.x\n",
        "        prev_theta = theta_i\n",
        "        f_next, reg = forecast_var_k(r_sub, theta_i, R=2, k=1)    # new output regime\n",
        "        \n",
        "        # ALSO A SEPARATE GARCH(1,1)\n",
        "        garch = arch_model(r_sub, vol='Garch', p=1, q=1, dist='normal', rescale= False).fit(disp='off')\n",
        "        yg_pred = garch.forecast(horizon=1).variance.values[-1, 0]\n",
        "        yg_roll.append(yg_pred)\n",
        "\n",
        "        regime.append(reg[-1])\n",
        "        f_roll.append(f_next[0])\n",
        "        globals()[f\"regime_{t}\"] = regime\n",
        "    \n",
        "    globals()[f\"yg_pred_{t}\"] = [yg_pred]\n",
        "\n",
        "\n",
        "    if len(f_roll) > 0:\n",
        "        f_roll = np.array(f_roll)\n",
        "        y_true = r[-len(f_roll):]**2\n",
        "        globals()[f\"y_true_{t}\"] = y_true\n",
        "        globals()[f\"y_pred_{t}\"] = f_roll\n",
        "        rolling_residuals[t] = y_true - f_roll\n",
        "\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.plot(r[-len(f_roll):]**2, label=\"Realized Var\", linewidth=1)\n",
        "    plt.plot(f_roll, label=\"Rolling 1-Step Forecast\", linewidth=1)\n",
        "    if len(regime) > 0 and np.array(regime).ndim ==2:\n",
        "        plt.plot(np.array(regime)[:,1] * 0.9 * plt.ylim()[1], color='black', alpha=0.6) \n",
        "        # Printing probability betweeen scaled 0 - 0.6\n",
        "    plt.title(f\"{t} - Rolling Forecast\"); plt.legend(); plt.tight_layout(); plt.show()"
      ],
      "id": "55c6508f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "def metrics(y_true, y_pred, eps=1e-10):\n",
        "    y_true = np.asarray(y_true, float)\n",
        "    y_pred = np.asarray(y_pred, float)\n",
        "    m = min(len(y_true), len(y_pred))\n",
        "    y_true, y_pred = y_true[-m:], np.clip(y_pred[-m:], eps, None)\n",
        "\n",
        "    mse  = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    # Gaussian log score per obs (constant included); many papers drop the constant\n",
        "    nll  = 0.5 * np.mean(np.log(2*np.pi*y_pred) + y_true / y_pred)\n",
        "\n",
        "    # Paper-comparable losses:\n",
        "    qlike   = np.mean(y_true / y_pred - np.log(y_true / y_pred + eps) - 1.0)\n",
        "    log_mse = np.mean((np.log(y_true + eps) - np.log(y_pred))**2)\n",
        "\n",
        "    bias = float(np.mean(y_true - y_pred))\n",
        "    return {\"RMSE\": rmse, \"MAE\": mae, \"NLL\": nll, \"QLIKE\": qlike, \"logMSE\": log_mse, \"Bias\": bias}\n",
        "\n",
        "results = {}\n",
        "results_garch = {}\n",
        "\n",
        "for t in tickers:\n",
        "    y_true = globals()[f\"y_true_{t}\"] \n",
        "    y_pred = globals()[f\"y_pred_{t}\"]\n",
        "    yg_pred = globals()[f\"yg_pred_{t}\"]\n",
        "    \n",
        "    results[t] = metrics(y_true, y_pred)\n",
        "    results_garch[t] = metrics(y_true, yg_pred)\n",
        "\n",
        "table = pd.DataFrame(results, index=[\"RMSE\",\"MAE\",\"NLL\",\"Mean\",\"QLIKE\", \"LogMSE\", \"Bias\"]).T.round(4)\n",
        "table_g = pd.DataFrame(results_garch, index=[\"RMSE\",\"MAE\",\"NLL\",\"Mean\",\"QLIKE\",\"LogMSE\",\"Bias\"]).T.round(4)\n",
        "\n",
        "# table = table.sort_values(by= \"RMSE\", ascending= True)\n",
        "\n",
        "print(\"MS-GARCH results:\\n\", table)\n",
        "print(\"\\nVanilla GARCH(1,1) results:\\n\", table_g)"
      ],
      "id": "e5605d4a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/shah/Desktop/code_document/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}